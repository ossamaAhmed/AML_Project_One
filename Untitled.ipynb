{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "#read the dataset\n",
    "import pandas as pd\n",
    "train_input = pd.read_csv(\"task1/X_train.csv\")\n",
    "train_output = pd.read_csv(\"task1/y_train.csv\")\n",
    "test_input = pd.read_csv(\"task1/X_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "887\n",
      "1212\n"
     ]
    }
   ],
   "source": [
    "num_of_train_samples = len(train_input)\n",
    "num_of_features = len(train_input.loc[0]) - 1 # first one is for id\n",
    "print( num_of_features)\n",
    "print( num_of_train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                id            x0            x1            x2           x3  \\\n",
      "count  1212.000000   1122.000000  1.140000e+03  1.132000e+03  1123.000000   \n",
      "mean    605.500000   7300.504957  1.003125e+06  1.051614e+06  1049.844772   \n",
      "std     350.018571   1379.891266  1.001817e+05  2.818085e+04    28.475255   \n",
      "min       0.000000   1030.502715  6.716345e+05  1.000037e+06  1000.062471   \n",
      "25%     302.750000   6496.988432  9.409699e+05  1.028118e+06  1025.913567   \n",
      "50%     605.500000   7381.752216  1.003238e+06  1.052406e+06  1050.174694   \n",
      "75%     908.250000   8153.767104  1.070372e+06  1.075329e+06  1074.864998   \n",
      "max    1211.000000  13055.814408  1.316548e+06  1.099990e+06  1099.845375   \n",
      "\n",
      "                x4             x5           x6             x7             x8  \\\n",
      "count  1133.000000    1126.000000  1141.000000    1122.000000    1122.000000   \n",
      "mean    105.047681  203511.156265  1050.735880  341958.172245  104916.372111   \n",
      "std       2.823009   29841.633207    28.623527   58820.438523    2755.013692   \n",
      "min     100.033879   63202.600024  1000.134779   92365.078214  100016.602565   \n",
      "25%     102.724769  186609.583069  1026.464126  309182.739540  102687.100342   \n",
      "50%     105.023063  201709.971057  1051.399190  337308.178918  104861.600927   \n",
      "75%     107.391464  220981.402036  1075.166305  371797.754187  107160.482832   \n",
      "max     110.048177  370398.522988  1099.997865  784817.830992  109991.914244   \n",
      "\n",
      "           ...               x877          x878          x879         x880  \\\n",
      "count      ...       1.127000e+03  1.127000e+03  1.132000e+03  1127.000000   \n",
      "mean       ...       3.825322e+11  1.003145e+06 -5.025956e+05  1001.891614   \n",
      "std        ...       4.347959e+11  9.594847e+04  8.608874e+04   100.410174   \n",
      "min        ...      -5.083882e+11  7.186635e+05 -1.110029e+06   643.042857   \n",
      "25%        ...       1.578603e+11  9.385117e+05 -5.475966e+05   933.591537   \n",
      "50%        ...       2.758197e+11  1.001974e+06 -4.965350e+05  1001.295903   \n",
      "75%        ...       4.867297e+11  1.063238e+06 -4.560057e+05  1069.927335   \n",
      "max        ...       7.405700e+12  1.308895e+06 -1.400403e+05  1323.073354   \n",
      "\n",
      "               x881         x882         x883         x884          x885  \\\n",
      "count  1.125000e+03  1127.000000  1148.000000  1126.000000   1123.000000   \n",
      "mean   9.995905e+05  3732.365716   100.659348  1617.956555  10505.966555   \n",
      "std    9.680491e+04   725.532171     9.336065   401.791865    290.648021   \n",
      "min    6.895354e+05   451.131089    65.692019   458.289896  10001.346875   \n",
      "25%    9.348641e+05  3297.203036    94.515998  1360.553119  10249.981685   \n",
      "50%    9.989761e+05  3768.931107   100.672131  1604.528424  10505.538263   \n",
      "75%    1.064618e+06  4179.602230   106.809319  1861.784028  10763.810688   \n",
      "max    1.276136e+06  6781.164024   126.678078  3745.022165  10999.908941   \n",
      "\n",
      "               x886  \n",
      "count   1111.000000  \n",
      "mean   65052.578569  \n",
      "std        0.029221  \n",
      "min    65052.528022  \n",
      "25%    65052.553207  \n",
      "50%    65052.579678  \n",
      "75%    65052.603107  \n",
      "max    65052.627907  \n",
      "\n",
      "[8 rows x 888 columns]\n",
      "                id            y\n",
      "count  1212.000000  1212.000000\n",
      "mean    605.500000    69.763201\n",
      "std     350.018571     9.941656\n",
      "min       0.000000    42.000000\n",
      "25%     302.750000    64.000000\n",
      "50%     605.500000    70.000000\n",
      "75%     908.250000    76.000000\n",
      "max    1211.000000    96.000000\n"
     ]
    }
   ],
   "source": [
    "print(train_input.describe())\n",
    "print(train_output.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(train_input['id'].dtype)\n",
    "train_input = train_input.sort_values(by=['id'])\n",
    "train_input = train_input.drop(columns=['id'])\n",
    "train_output = train_output.sort_values(by=['id'])\n",
    "train_output = train_output.drop(columns=['id'])\n",
    "test_input = test_input.sort_values(by=['id'])\n",
    "test_input = test_input.drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seeing whats the total number of NaNs per feature\n",
    "((train_input.isna().sum() / num_of_features * 100) > 20).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILLING MISSING VALUES with the mean\n",
    "average_per_feature = train_input.mean()\n",
    "train_input = train_input.fillna(average_per_feature)\n",
    "test_input = test_input.fillna(average_per_feature)\n",
    "train_output = train_output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1212,)\n",
      "(1212, 887)\n",
      "(776, 887)\n"
     ]
    }
   ],
   "source": [
    "original_train_output_shape = train_output.shape\n",
    "original_train_input_shape = train_input.shape\n",
    "original_test_input_shape = test_input.shape\n",
    "print(train_output.shape)\n",
    "print(train_input.shape)\n",
    "print(test_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(969, 887)\n",
      "(243, 887)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_input, train_output, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 11  25  30  31  44  53  60  63  80  91  99 106 111 119 120 123 126 137\n",
      " 142 144 145 154 158 159 161 164 181 185 188 190 192 194 196 201 202 205\n",
      " 208 209 213 232 234 237 246 253 265 276 290 291 297 309 320 328 329 330\n",
      " 342 344 345 353 361 370 376 377 378 390 392 407 418 422 426 428 430 432\n",
      " 447 451 452 460 464 465 470 478 480 481 482 485 488 490 493 495 496 497\n",
      " 505 507 511 515 518 523 526 528 531 533 538 542 551 554 555 559 562 564\n",
      " 585 586 591 596 604 605 608 615 622 623 624 634 635 636 637 642 646 647\n",
      " 651 660 671 672 679 684 686 698 715 720 721 731 734 735 736 739 744 745\n",
      " 751 755 759 770 772 781 799 805 810 815 816 819 824 826 838 848 852 853\n",
      " 858 861 870 886]\n"
     ]
    }
   ],
   "source": [
    "#Remove low variance features\n",
    "indecies_w_low_var = np.where((X_train.var() < 10e-2) == True)[0]\n",
    "print(indecies_w_low_var)\n",
    "X_train = np.delete(X_train.values, indecies_w_low_var, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:1706: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97,)\n",
      "(872, 721)\n"
     ]
    }
   ],
   "source": [
    "#OUTLIER DETECTION (maybe do it after normalization)\n",
    "#OUTLIER DETECTION 1\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "import collections\n",
    "# Isolation Forest ----\n",
    "# training the model\n",
    "clf = IsolationForest()\n",
    "clf.fit(X_train)\n",
    "# predictions\n",
    "outlier_detection = clf.predict(X_train)\n",
    "collections.Counter(outlier_detection)\n",
    "indexes = np.where(outlier_detection == -1)\n",
    "print(indexes[0].shape)\n",
    "X_train_wo_outliers = np.delete(X_train, indexes[0], axis=0)\n",
    "y_train_wo_outliers = np.delete(y_train.values, indexes[0], axis=0)\n",
    "print(X_train_wo_outliers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:1706: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "outlier_detection_2 = LocalOutlierFactor(n_neighbors=40, n_jobs=-1).fit_predict(X_train)\n",
    "indexes_2 = np.where(outlier_detection == -1)\n",
    "print(indexes_2[0].shape)\n",
    "#SAME NUMBER OF DATA ROWS ELIMINATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_wo_outliers)\n",
    "X_train_scaled_wo = scaler.transform(X_train_wo_outliers)\n",
    "X_test = np.delete(X_test.values, indecies_w_low_var, axis=1)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.81202448293061\n",
      "-6.710124570648415\n",
      "22.68814725509541\n",
      "-5.342570201198807\n"
     ]
    }
   ],
   "source": [
    "print(np.max(X_train_scaled_wo))\n",
    "print(np.min(X_train_scaled_wo))\n",
    "print(np.max(X_test_scaled))\n",
    "print(np.min(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(872, 200)\n"
     ]
    }
   ],
   "source": [
    "#FEATURE SELECTION\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "feature_selector = SelectKBest(f_regression, k=200)\n",
    "feature_selector.fit(X_train_scaled_wo, y_train_wo_outliers)\n",
    "X_train_scaled_wo_reduced = feature_selector.transform(X_train_scaled_wo)\n",
    "# X_train_scaled_wo_reduced = SelectKBest(f_regression, k=200).fit_transform(X_train_scaled_wo, y_train_wo_outliers)\n",
    "print(X_train_scaled_wo_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #FESTURE SELECTION 2\n",
    "# from sklearn.decomposition import PCA\n",
    "# # pca = PCA()\n",
    "# # pca.fit(train_input_scaled)\n",
    "# # X_train_reduced = pca.transform(train_input_scaled)\n",
    "# # X_test_reduced = pca.transform(X_test)\n",
    "# # test_input_scaled_reduced = pca.transform(test_input_scaled)\n",
    "\n",
    "# X_train_reduced = train_input_scaled\n",
    "# X_test_reduced = train_input_scaled\n",
    "# test_input_scaled_reduced = test_input_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL BUILDING\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support, r2_score\n",
    "\n",
    "# ELASTIC NET MODEL\n",
    "# model = ElasticNet()\n",
    "# parameters = {'alpha': [0.25, 0.5, 1], 'l1_ratio':[0.5, 0.7, 1], 'fit_intercept':[True, False], 'max_iter': [10000]}\n",
    "#Random Forests\n",
    "# model = RandomForestRegressor()\n",
    "# parameters = {'n_estimators': [50, 60, 70, 80, 90, 100], 'oob_score':[True], 'bootstrap':[True]}\n",
    "\n",
    "#SVR MODEL\n",
    "# model = SVR()\n",
    "# C_range = np.logspace(-2, 2, 5)\n",
    "# gamma_range = np.logspace(-6, 6, 5)\n",
    "# epsilon_range = np.logspace(-3, 2, 6)\n",
    "# parameters = dict(gamma=gamma_range, C=C_range, epsilon=epsilon_range)\n",
    "\n",
    "#GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,\n",
    "#...     max_depth=1, random_state=0, loss='ls')\n",
    "# model = GradientBoostingRegressor()\n",
    "# parameters = {'n_estimators': [50, 100, 150, 200, 2000, 1000], 'learning_rate': [0.05, 0.1, 0.25]}\n",
    "\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "model = XGBRegressor()\n",
    "parameters = {'n_estimators': [100, 200, 2000, 1000, 2250, 2500], 'learning_rate': [0.05, 0.1, 0.25]}\n",
    "\n",
    "grid = GridSearchCV(model, parameters, scoring='r2', cv=5, verbose=3)\n",
    "\n",
    "# kf = KFold(n_splits=8)\n",
    "# accuracies = []\n",
    "# r_scores = []\n",
    "# for train_index, test_index in kf.split(train_input_normalized_1, train_output):\n",
    "#     X_train, X_test = train_input_normalized_1[train_index], train_input_normalized_1[test_index]\n",
    "#     y_train, y_test = train_output_normalized_1[train_index], train_output_normalized_1[test_index]\n",
    "#     lm = SVR(kernel='rbf', degree=6, C=1e3, gamma=0.1)\n",
    "#     #lm = ElasticNet(alpha=0.5, l1_ratio=0.25, max_iter=1000)\n",
    "#     #lm = ElasticNet(alpha=0.25, copy_X=True, fit_intercept=False, l1_ratio=0.5,\n",
    "# #       max_iter=10000, normalize=False, positive=False, precompute=False,\n",
    "# #       random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
    "#     lm.fit(X_train, y_train)\n",
    "#     y_pred = lm.predict(X_test)\n",
    "#     r_score = r2_score(y_test, y_pred)\n",
    "#     r_scores.append(r_score)\n",
    "#     #accuracies.append(accuracy_score(y_test, y_pred))\n",
    "# #print(\"Accuracy %d\".format(np.mean(accuracies)))\n",
    "# print(np.mean(r_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [100, 200, 2000, 1000, 2250, 2500], 'learning_rate': [0.05, 0.1, 0.25]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='r2', verbose=3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "[CV] learning_rate=0.05, n_estimators=100 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=100, score=0.4375874024885543, total=   0.8s\n",
      "[CV] learning_rate=0.05, n_estimators=100 ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=0.05, n_estimators=100, score=0.3761119342008473, total=   0.8s\n",
      "[CV] learning_rate=0.05, n_estimators=100 ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    1.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=0.05, n_estimators=100, score=0.47216292619265166, total=   0.8s\n",
      "[CV] learning_rate=0.05, n_estimators=100 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=100, score=0.5082592142372175, total=   0.8s\n",
      "[CV] learning_rate=0.05, n_estimators=100 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=100, score=0.524879661645214, total=   0.8s\n",
      "[CV] learning_rate=0.05, n_estimators=200 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=200, score=0.45538884434092775, total=   1.6s\n",
      "[CV] learning_rate=0.05, n_estimators=200 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=200, score=0.3662189283849482, total=   2.1s\n",
      "[CV] learning_rate=0.05, n_estimators=200 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=200, score=0.49433993922463026, total=   2.8s\n",
      "[CV] learning_rate=0.05, n_estimators=200 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=200, score=0.5372779912435031, total=   2.6s\n",
      "[CV] learning_rate=0.05, n_estimators=200 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=200, score=0.5325080416861594, total=   2.6s\n",
      "[CV] learning_rate=0.05, n_estimators=2000 ...........................\n",
      "[CV]  learning_rate=0.05, n_estimators=2000, score=0.45063757388159076, total=  27.0s\n",
      "[CV] learning_rate=0.05, n_estimators=2000 ...........................\n",
      "[CV]  learning_rate=0.05, n_estimators=2000, score=0.3571554343759016, total=  27.5s\n",
      "[CV] learning_rate=0.05, n_estimators=2000 ...........................\n",
      "[CV]  learning_rate=0.05, n_estimators=2000, score=0.5157763208948709, total=  27.0s\n",
      "[CV] learning_rate=0.05, n_estimators=2000 ...........................\n",
      "[CV]  learning_rate=0.05, n_estimators=2000, score=0.5672681848415979, total=  27.5s\n",
      "[CV] learning_rate=0.05, n_estimators=2000 ...........................\n",
      "[CV]  learning_rate=0.05, n_estimators=2000, score=0.5411630055708695, total=  27.6s\n",
      "[CV] learning_rate=0.05, n_estimators=1000 ...........................\n",
      "[CV]  learning_rate=0.05, n_estimators=1000, score=0.45119130789155804, total=  13.9s\n",
      "[CV] learning_rate=0.05, n_estimators=1000 ...........................\n",
      "[CV]  learning_rate=0.05, n_estimators=1000, score=0.3570961179433473, total=  13.9s\n",
      "[CV] learning_rate=0.05, n_estimators=1000 ...........................\n",
      "[CV]  learning_rate=0.05, n_estimators=1000, score=0.5148561890177423, total=  14.2s\n",
      "[CV] learning_rate=0.05, n_estimators=1000 ...........................\n",
      "[CV]  learning_rate=0.05, n_estimators=1000, score=0.565694190632535, total=  14.0s\n",
      "[CV] learning_rate=0.05, n_estimators=1000 ...........................\n",
      "[CV]  learning_rate=0.05, n_estimators=1000, score=0.5410278370714698, total=  13.6s\n",
      "[CV] learning_rate=0.05, n_estimators=2250 ...........................\n",
      "[CV]  learning_rate=0.05, n_estimators=2250, score=0.4506204952534858, total=  30.8s\n",
      "[CV] learning_rate=0.05, n_estimators=2250 ...........................\n",
      "[CV]  learning_rate=0.05, n_estimators=2250, score=0.35704961079339304, total=  30.5s\n",
      "[CV] learning_rate=0.05, n_estimators=2250 ...........................\n",
      "[CV]  learning_rate=0.05, n_estimators=2250, score=0.5158885496770329, total=  30.4s\n",
      "[CV] learning_rate=0.05, n_estimators=2250 ...........................\n",
      "[CV]  learning_rate=0.05, n_estimators=2250, score=0.5673381402347617, total=  30.4s\n",
      "[CV] learning_rate=0.05, n_estimators=2250 ...........................\n",
      "[CV]  learning_rate=0.05, n_estimators=2250, score=0.5411204588133212, total=  30.8s\n",
      "[CV] learning_rate=0.05, n_estimators=2500 ...........................\n",
      "[CV]  learning_rate=0.05, n_estimators=2500, score=0.4506593715355882, total=  34.6s\n",
      "[CV] learning_rate=0.05, n_estimators=2500 ...........................\n",
      "[CV]  learning_rate=0.05, n_estimators=2500, score=0.35703362224953994, total=  33.5s\n",
      "[CV] learning_rate=0.05, n_estimators=2500 ...........................\n"
     ]
    }
   ],
   "source": [
    "grid.fit(X_train_scaled_wo_reduced, y_train_wo_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST ON TESTING DATASET AVAILABLE\n",
    "#feature selection\n",
    "X_test_scaled_reduced = feature_selector.transform(X_test_scaled)\n",
    "#predict now\n",
    "y_pred = grid.predict(X_test_scaled_reduced)\n",
    "r_score = r2_score(y_test, y_pred)\n",
    "print(r_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "rfecv = RFECV(estimator=grid.best_estimator_, step=1, cv=10,\n",
    "              scoring='r2', verbose=0)\n",
    "rfecv.fit(X_train_scaled_wo_reduced, y_train_wo_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv.n_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rfecv.predict(X_test_scaled_reduced)\n",
    "r_score = r2_score(y_test, y_pred)\n",
    "print(r_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST ON TESTING DATASET RESERVED\n",
    "#SCALE\n",
    "test_input = np.delete(test_input.values, indecies_w_low_var, axis=1)\n",
    "test_input_scaled = scaler.transform(test_input)\n",
    "#feature selection\n",
    "test_input_scaled_reduced = feature_selector.transform(test_input_scaled)\n",
    "#predict now\n",
    "test_input_pred = grid.predict(test_input_scaled_reduced) #GRID OR RFECV depends\n",
    "test_input_pred = test_input_pred.squeeze()\n",
    "test_input_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the output\n",
    "predicted_output = {'y': test_input_pred}\n",
    "predicted_output_df = pd.DataFrame(data=predicted_output)\n",
    "predicted_output_df.to_csv(\"task1/y_test.csv\", index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUBMISSION 51 local score TRAIN: ~ 0.50 TEST: ~ 0.57 REAL TEST: 0.665\n",
    "#SUBMISSION 52 local score TRAIN: ~  0.5 TEST: 0.64 ~  REAL TEST: 0.65\n",
    "#SUBMISSION 53 local score TRAIN: ~  TEST:  ~  REAL TEST: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
