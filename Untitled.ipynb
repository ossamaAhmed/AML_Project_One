{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "#read the dataset\n",
    "import pandas as pd\n",
    "train_input = pd.read_csv(\"task1/X_train.csv\")\n",
    "train_output = pd.read_csv(\"task1/y_train.csv\")\n",
    "test_input = pd.read_csv(\"task1/X_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "887\n",
      "1212\n"
     ]
    }
   ],
   "source": [
    "num_of_train_samples = len(train_input)\n",
    "num_of_features = len(train_input.loc[0]) - 1 # first one is for id\n",
    "print( num_of_features)\n",
    "print( num_of_train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                id            x0            x1            x2           x3  \\\n",
      "count  1212.000000   1122.000000  1.140000e+03  1.132000e+03  1123.000000   \n",
      "mean    605.500000   7300.504957  1.003125e+06  1.051614e+06  1049.844772   \n",
      "std     350.018571   1379.891266  1.001817e+05  2.818085e+04    28.475255   \n",
      "min       0.000000   1030.502715  6.716345e+05  1.000037e+06  1000.062471   \n",
      "25%     302.750000   6496.988432  9.409699e+05  1.028118e+06  1025.913567   \n",
      "50%     605.500000   7381.752216  1.003238e+06  1.052406e+06  1050.174694   \n",
      "75%     908.250000   8153.767104  1.070372e+06  1.075329e+06  1074.864998   \n",
      "max    1211.000000  13055.814408  1.316548e+06  1.099990e+06  1099.845375   \n",
      "\n",
      "                x4             x5           x6             x7             x8  \\\n",
      "count  1133.000000    1126.000000  1141.000000    1122.000000    1122.000000   \n",
      "mean    105.047681  203511.156265  1050.735880  341958.172245  104916.372111   \n",
      "std       2.823009   29841.633207    28.623527   58820.438523    2755.013692   \n",
      "min     100.033879   63202.600024  1000.134779   92365.078214  100016.602565   \n",
      "25%     102.724769  186609.583069  1026.464126  309182.739540  102687.100342   \n",
      "50%     105.023063  201709.971057  1051.399190  337308.178918  104861.600927   \n",
      "75%     107.391464  220981.402036  1075.166305  371797.754187  107160.482832   \n",
      "max     110.048177  370398.522988  1099.997865  784817.830992  109991.914244   \n",
      "\n",
      "           ...               x877          x878          x879         x880  \\\n",
      "count      ...       1.127000e+03  1.127000e+03  1.132000e+03  1127.000000   \n",
      "mean       ...       3.825322e+11  1.003145e+06 -5.025956e+05  1001.891614   \n",
      "std        ...       4.347959e+11  9.594847e+04  8.608874e+04   100.410174   \n",
      "min        ...      -5.083882e+11  7.186635e+05 -1.110029e+06   643.042857   \n",
      "25%        ...       1.578603e+11  9.385117e+05 -5.475966e+05   933.591537   \n",
      "50%        ...       2.758197e+11  1.001974e+06 -4.965350e+05  1001.295903   \n",
      "75%        ...       4.867297e+11  1.063238e+06 -4.560057e+05  1069.927335   \n",
      "max        ...       7.405700e+12  1.308895e+06 -1.400403e+05  1323.073354   \n",
      "\n",
      "               x881         x882         x883         x884          x885  \\\n",
      "count  1.125000e+03  1127.000000  1148.000000  1126.000000   1123.000000   \n",
      "mean   9.995905e+05  3732.365716   100.659348  1617.956555  10505.966555   \n",
      "std    9.680491e+04   725.532171     9.336065   401.791865    290.648021   \n",
      "min    6.895354e+05   451.131089    65.692019   458.289896  10001.346875   \n",
      "25%    9.348641e+05  3297.203036    94.515998  1360.553119  10249.981685   \n",
      "50%    9.989761e+05  3768.931107   100.672131  1604.528424  10505.538263   \n",
      "75%    1.064618e+06  4179.602230   106.809319  1861.784028  10763.810688   \n",
      "max    1.276136e+06  6781.164024   126.678078  3745.022165  10999.908941   \n",
      "\n",
      "               x886  \n",
      "count   1111.000000  \n",
      "mean   65052.578569  \n",
      "std        0.029221  \n",
      "min    65052.528022  \n",
      "25%    65052.553207  \n",
      "50%    65052.579678  \n",
      "75%    65052.603107  \n",
      "max    65052.627907  \n",
      "\n",
      "[8 rows x 888 columns]\n",
      "                id            y\n",
      "count  1212.000000  1212.000000\n",
      "mean    605.500000    69.763201\n",
      "std     350.018571     9.941656\n",
      "min       0.000000    42.000000\n",
      "25%     302.750000    64.000000\n",
      "50%     605.500000    70.000000\n",
      "75%     908.250000    76.000000\n",
      "max    1211.000000    96.000000\n"
     ]
    }
   ],
   "source": [
    "print(train_input.describe())\n",
    "print(train_output.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(train_input['id'].dtype)\n",
    "train_input = train_input.sort_values(by=['id'])\n",
    "train_input = train_input.drop(columns=['id'])\n",
    "train_output = train_output.sort_values(by=['id'])\n",
    "train_output = train_output.drop(columns=['id'])\n",
    "test_input = test_input.sort_values(by=['id'])\n",
    "test_input = test_input.drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_per_feature = train_input.mean()\n",
    "train_input = train_input.fillna(average_per_feature)\n",
    "train_input.loc[0][16]\n",
    "test_input = test_input.fillna(average_per_feature)\n",
    "train_output = train_output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1212,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize data\n",
    "# train_input_min = train_input.min()\n",
    "# train_input_max = train_input.max()\n",
    "# train_input_normalized = (train_input - train_input_min)/(train_input_max - train_input_min)\n",
    "# test_input_normalized = (test_input - train_input_min)/(train_input_max - train_input_min)\n",
    "# print (train_input_min, train_input_max)\n",
    "# from sklearn import preprocessing\n",
    "# train_input_normalized_1 = preprocessing.scale(train_input)\n",
    "# test_input_normalized_1 = preprocessing.scale(test_input)\n",
    "# train_output_normalized_1 = preprocessing.scale(train_output)\n",
    "# train_output_min = train_output.min()\n",
    "# train_output_max = train_output.max()\n",
    "# train_output_normalized = (train_output - train_output_min)/(train_output_max - train_output_min)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_input)\n",
    "train_input_scaled = scaler.transform(train_input)\n",
    "test_input_scaled = scaler.transform(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1212, 887)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_input_scaled, train_output, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "#FESTURE SELECTION\n",
    "from sklearn.decomposition import PCA\n",
    "# pca = PCA()\n",
    "# pca.fit(train_input_scaled)\n",
    "# X_train_reduced = pca.transform(train_input_scaled)\n",
    "# X_test_reduced = pca.transform(X_test)\n",
    "# test_input_scaled_reduced = pca.transform(test_input_scaled)\n",
    "\n",
    "X_train_reduced = train_input_scaled\n",
    "X_test_reduced = train_input_scaled\n",
    "test_input_scaled_reduced = test_input_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:1706: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    }
   ],
   "source": [
    "#OUTLIER DETECTION\n",
    "from sklearn.ensemble import IsolationForest\n",
    "# Isolation Forest ----\n",
    "\n",
    "# training the model\n",
    "clf = IsolationForest()\n",
    "clf.fit(X_train_reduced)\n",
    "\n",
    "# predictions\n",
    "y_pred_train = clf.predict(X_train_reduced)\n",
    "# y_pred_test = clf.predict(X_test)\n",
    "# y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "collections.Counter(y_pred_train)\n",
    "indexes = np.where(y_pred_train == -1)\n",
    "indexes[0].shape\n",
    "train_input_scaled_wo_outliers = np.delete(train_input_scaled, indexes[0], axis=0)\n",
    "train_output_wo_outliers = np.delete(train_output.data, indexes[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(776, 887)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support, r2_score\n",
    "\n",
    "# ELASTIC NET MODEL\n",
    "# model = ElasticNet()\n",
    "# parameters = {'alpha': [0.25, 0.5, 1], 'l1_ratio':[0.5, 0.7, 1], 'fit_intercept':[True, False], 'max_iter': [10000]}\n",
    "#Random Forests\n",
    "# model = RandomForestRegressor()\n",
    "# parameters = {'n_estimators': [50, 60, 70, 80, 90, 100], 'oob_score':[True], 'bootstrap':[True]}\n",
    "\n",
    "#SVR MODEL\n",
    "# model = SVR()\n",
    "# C_range = np.logspace(-2, 2, 5)\n",
    "# gamma_range = np.logspace(-6, 6, 5)\n",
    "# epsilon_range = np.logspace(-3, 2, 6)\n",
    "# parameters = dict(gamma=gamma_range, C=C_range, epsilon=epsilon_range)\n",
    "\n",
    "#GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,\n",
    "#...     max_depth=1, random_state=0, loss='ls')\n",
    "model = GradientBoostingRegressor()\n",
    "parameters = {'n_estimators': [50, 100, 150, 200], 'learning_rate': [0.05, 0.1, 0.25]}\n",
    "\n",
    "grid = GridSearchCV(model, parameters, scoring='r2', cv=5, verbose=3)\n",
    "\n",
    "# kf = KFold(n_splits=8)\n",
    "# accuracies = []\n",
    "# r_scores = []\n",
    "# for train_index, test_index in kf.split(train_input_normalized_1, train_output):\n",
    "#     X_train, X_test = train_input_normalized_1[train_index], train_input_normalized_1[test_index]\n",
    "#     y_train, y_test = train_output_normalized_1[train_index], train_output_normalized_1[test_index]\n",
    "#     lm = SVR(kernel='rbf', degree=6, C=1e3, gamma=0.1)\n",
    "#     #lm = ElasticNet(alpha=0.5, l1_ratio=0.25, max_iter=1000)\n",
    "#     #lm = ElasticNet(alpha=0.25, copy_X=True, fit_intercept=False, l1_ratio=0.5,\n",
    "# #       max_iter=10000, normalize=False, positive=False, precompute=False,\n",
    "# #       random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
    "#     lm.fit(X_train, y_train)\n",
    "#     y_pred = lm.predict(X_test)\n",
    "#     r_score = r2_score(y_test, y_pred)\n",
    "#     r_scores.append(r_score)\n",
    "#     #accuracies.append(accuracy_score(y_test, y_pred))\n",
    "# #print(\"Accuracy %d\".format(np.mean(accuracies)))\n",
    "# print(np.mean(r_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV] learning_rate=0.05, n_estimators=50 .............................\n",
      "[CV]  learning_rate=0.05, n_estimators=50, score=0.39763402490334343, total=   3.2s\n",
      "[CV] learning_rate=0.05, n_estimators=50 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=0.05, n_estimators=50, score=0.38346614929435374, total=   3.1s\n",
      "[CV] learning_rate=0.05, n_estimators=50 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    6.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=0.05, n_estimators=50, score=0.4310799015607485, total=   3.1s\n",
      "[CV] learning_rate=0.05, n_estimators=50 .............................\n",
      "[CV]  learning_rate=0.05, n_estimators=50, score=0.5170797481098255, total=   3.2s\n",
      "[CV] learning_rate=0.05, n_estimators=50 .............................\n",
      "[CV]  learning_rate=0.05, n_estimators=50, score=0.5477364213066627, total=   3.1s\n",
      "[CV] learning_rate=0.05, n_estimators=100 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=100, score=0.4334003958087399, total=   6.0s\n",
      "[CV] learning_rate=0.05, n_estimators=100 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=100, score=0.41153746161560345, total=   6.1s\n",
      "[CV] learning_rate=0.05, n_estimators=100 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=100, score=0.4707455809371901, total=   6.1s\n",
      "[CV] learning_rate=0.05, n_estimators=100 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=100, score=0.5388940292892568, total=   6.0s\n",
      "[CV] learning_rate=0.05, n_estimators=100 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=100, score=0.5856419136354123, total=   6.0s\n",
      "[CV] learning_rate=0.05, n_estimators=150 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=150, score=0.44661937987679945, total=   8.7s\n",
      "[CV] learning_rate=0.05, n_estimators=150 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=150, score=0.4020335982658848, total=   8.9s\n",
      "[CV] learning_rate=0.05, n_estimators=150 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=150, score=0.4620126834683581, total=   8.9s\n",
      "[CV] learning_rate=0.05, n_estimators=150 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=150, score=0.547002220220322, total=   8.6s\n",
      "[CV] learning_rate=0.05, n_estimators=150 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=150, score=0.5994903065627641, total=   8.7s\n",
      "[CV] learning_rate=0.05, n_estimators=200 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=200, score=0.4644317127014831, total=  11.4s\n",
      "[CV] learning_rate=0.05, n_estimators=200 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=200, score=0.41833610185608816, total=  11.6s\n",
      "[CV] learning_rate=0.05, n_estimators=200 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=200, score=0.4537926498075152, total=  11.6s\n",
      "[CV] learning_rate=0.05, n_estimators=200 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=200, score=0.5460641970928641, total=  11.4s\n",
      "[CV] learning_rate=0.05, n_estimators=200 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=200, score=0.6016231241168528, total=  11.4s\n",
      "[CV] learning_rate=0.1, n_estimators=50 ..............................\n",
      "[CV]  learning_rate=0.1, n_estimators=50, score=0.42685126861157296, total=   3.1s\n",
      "[CV] learning_rate=0.1, n_estimators=50 ..............................\n",
      "[CV]  learning_rate=0.1, n_estimators=50, score=0.405068499457398, total=   3.0s\n",
      "[CV] learning_rate=0.1, n_estimators=50 ..............................\n",
      "[CV]  learning_rate=0.1, n_estimators=50, score=0.43340142930911607, total=   3.0s\n",
      "[CV] learning_rate=0.1, n_estimators=50 ..............................\n",
      "[CV]  learning_rate=0.1, n_estimators=50, score=0.5122201681432095, total=   3.0s\n",
      "[CV] learning_rate=0.1, n_estimators=50 ..............................\n",
      "[CV]  learning_rate=0.1, n_estimators=50, score=0.5638218129312718, total=   3.0s\n",
      "[CV] learning_rate=0.1, n_estimators=100 .............................\n",
      "[CV]  learning_rate=0.1, n_estimators=100, score=0.4548492008068675, total=   5.8s\n",
      "[CV] learning_rate=0.1, n_estimators=100 .............................\n",
      "[CV]  learning_rate=0.1, n_estimators=100, score=0.4157383594921845, total=   5.7s\n",
      "[CV] learning_rate=0.1, n_estimators=100 .............................\n",
      "[CV]  learning_rate=0.1, n_estimators=100, score=0.4180518035972558, total=   5.8s\n",
      "[CV] learning_rate=0.1, n_estimators=100 .............................\n",
      "[CV]  learning_rate=0.1, n_estimators=100, score=0.5303307350511016, total=   5.7s\n",
      "[CV] learning_rate=0.1, n_estimators=100 .............................\n",
      "[CV]  learning_rate=0.1, n_estimators=100, score=0.5786297234223527, total=   5.8s\n",
      "[CV] learning_rate=0.1, n_estimators=150 .............................\n",
      "[CV]  learning_rate=0.1, n_estimators=150, score=0.4530920663954191, total=   9.3s\n",
      "[CV] learning_rate=0.1, n_estimators=150 .............................\n",
      "[CV]  learning_rate=0.1, n_estimators=150, score=0.4257106988656838, total=   8.7s\n",
      "[CV] learning_rate=0.1, n_estimators=150 .............................\n",
      "[CV]  learning_rate=0.1, n_estimators=150, score=0.42693434144228914, total=   8.5s\n",
      "[CV] learning_rate=0.1, n_estimators=150 .............................\n",
      "[CV]  learning_rate=0.1, n_estimators=150, score=0.5317348470259866, total=   8.4s\n",
      "[CV] learning_rate=0.1, n_estimators=150 .............................\n",
      "[CV]  learning_rate=0.1, n_estimators=150, score=0.5577605904752467, total=   8.5s\n",
      "[CV] learning_rate=0.1, n_estimators=200 .............................\n",
      "[CV]  learning_rate=0.1, n_estimators=200, score=0.45547733062234874, total=  11.2s\n",
      "[CV] learning_rate=0.1, n_estimators=200 .............................\n",
      "[CV]  learning_rate=0.1, n_estimators=200, score=0.4163856077049798, total=  11.2s\n",
      "[CV] learning_rate=0.1, n_estimators=200 .............................\n",
      "[CV]  learning_rate=0.1, n_estimators=200, score=0.40637507041470544, total=  11.2s\n",
      "[CV] learning_rate=0.1, n_estimators=200 .............................\n",
      "[CV]  learning_rate=0.1, n_estimators=200, score=0.5093799805076668, total=  11.1s\n",
      "[CV] learning_rate=0.1, n_estimators=200 .............................\n",
      "[CV]  learning_rate=0.1, n_estimators=200, score=0.5615116031364106, total=  11.2s\n",
      "[CV] learning_rate=0.25, n_estimators=50 .............................\n",
      "[CV]  learning_rate=0.25, n_estimators=50, score=0.41703663265441704, total=   2.9s\n",
      "[CV] learning_rate=0.25, n_estimators=50 .............................\n",
      "[CV]  learning_rate=0.25, n_estimators=50, score=0.32495149436291526, total=   3.0s\n",
      "[CV] learning_rate=0.25, n_estimators=50 .............................\n",
      "[CV]  learning_rate=0.25, n_estimators=50, score=0.41758586010065, total=   2.9s\n",
      "[CV] learning_rate=0.25, n_estimators=50 .............................\n",
      "[CV]  learning_rate=0.25, n_estimators=50, score=0.5149165245454548, total=   2.9s\n",
      "[CV] learning_rate=0.25, n_estimators=50 .............................\n",
      "[CV]  learning_rate=0.25, n_estimators=50, score=0.4734089095848871, total=   2.9s\n",
      "[CV] learning_rate=0.25, n_estimators=100 ............................\n",
      "[CV]  learning_rate=0.25, n_estimators=100, score=0.42419156702094407, total=   5.6s\n",
      "[CV] learning_rate=0.25, n_estimators=100 ............................\n",
      "[CV]  learning_rate=0.25, n_estimators=100, score=0.33056285002921026, total=   5.6s\n",
      "[CV] learning_rate=0.25, n_estimators=100 ............................\n",
      "[CV]  learning_rate=0.25, n_estimators=100, score=0.41143050721393726, total=   5.6s\n",
      "[CV] learning_rate=0.25, n_estimators=100 ............................\n",
      "[CV]  learning_rate=0.25, n_estimators=100, score=0.4747064592724508, total=   5.7s\n",
      "[CV] learning_rate=0.25, n_estimators=100 ............................\n",
      "[CV]  learning_rate=0.25, n_estimators=100, score=0.4467024225370232, total=   5.7s\n",
      "[CV] learning_rate=0.25, n_estimators=150 ............................\n",
      "[CV]  learning_rate=0.25, n_estimators=150, score=0.41118188762146823, total=   8.2s\n",
      "[CV] learning_rate=0.25, n_estimators=150 ............................\n",
      "[CV]  learning_rate=0.25, n_estimators=150, score=0.32820278535701264, total=   8.3s\n",
      "[CV] learning_rate=0.25, n_estimators=150 ............................\n",
      "[CV]  learning_rate=0.25, n_estimators=150, score=0.41706900767766253, total=   8.3s\n",
      "[CV] learning_rate=0.25, n_estimators=150 ............................\n",
      "[CV]  learning_rate=0.25, n_estimators=150, score=0.5041928023946296, total=   8.3s\n",
      "[CV] learning_rate=0.25, n_estimators=150 ............................\n",
      "[CV]  learning_rate=0.25, n_estimators=150, score=0.4511024320918945, total=   8.4s\n",
      "[CV] learning_rate=0.25, n_estimators=200 ............................\n",
      "[CV]  learning_rate=0.25, n_estimators=200, score=0.4042716919342706, total=  11.0s\n",
      "[CV] learning_rate=0.25, n_estimators=200 ............................\n",
      "[CV]  learning_rate=0.25, n_estimators=200, score=0.33240895927484515, total=  10.9s\n",
      "[CV] learning_rate=0.25, n_estimators=200 ............................\n",
      "[CV]  learning_rate=0.25, n_estimators=200, score=0.42571521976234084, total=  11.3s\n",
      "[CV] learning_rate=0.25, n_estimators=200 ............................\n",
      "[CV]  learning_rate=0.25, n_estimators=200, score=0.5084498150314082, total=  12.0s\n",
      "[CV] learning_rate=0.25, n_estimators=200 ............................\n",
      "[CV]  learning_rate=0.25, n_estimators=200, score=0.45053800970279, total=  11.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:  7.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [50, 100, 150, 200], 'learning_rate': [0.05, 0.1, 0.25]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='r2', verbose=3)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(train_input_scaled_wo_outliers, train_output_wo_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.05, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=1,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=200, presort='auto', random_state=None,\n",
      "             subsample=1.0, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1090, 887)\n",
      "(776, 887)\n"
     ]
    }
   ],
   "source": [
    "print(train_input_scaled_wo_outliers.shape)\n",
    "print(test_input_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "rfecv = RFECV(estimator=grid.best_estimator_, step=1, cv=10,\n",
    "              scoring='r2', verbose=0)\n",
    "rfecv.fit(train_input_scaled_wo_outliers, train_output_wo_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv.n_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [122, 1212]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-46743906f8b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#predict now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_reduced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mr_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36mr2_score\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \"\"\"\n\u001b[1;32m    529\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 530\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \"\"\"\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 204\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [122, 1212]"
     ]
    }
   ],
   "source": [
    "#predict now\n",
    "y_pred = grid.predict(X_test_reduced)\n",
    "r_score = r2_score(y_test, y_pred)\n",
    "print(r_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(776,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_output = grid.predict(test_input_scaled)\n",
    "predicted_output = predicted_output.squeeze()\n",
    "predicted_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the output\n",
    "predicted_output = {'y': predicted_output}\n",
    "# predicted_output['id'].append(2)\n",
    "# predicted_output['y'].append(67.8)\n",
    "# predicted_output['id'].append(4)\n",
    "# predicted_output['y'].append(68.9)\n",
    "predicted_output_df = pd.DataFrame(data=predicted_output)\n",
    "predicted_output_df.to_csv(\"task1/y_test.csv\", index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
